## 1. 赛题理解

###### 1 赛题背景

```python
1. 数据说明
2. 核心特征
3. 可调参数
4. 采集时间
```

###### 2 赛题目标

```python
1. 想要的预测结果
```

###### 3 数据概览

```python
1. 数据量
2. 类别
3. 属性
```

###### 4 评估指标

```python
1. 回归：mse
2. 分类：F1 score
```

###### 5 赛题模型

```python
1. 分类模型：决策树、随机森林
2. 回归模型：线性回归
```

## 2.数据探索

#### 1理论知识

###### 1 变量识别

```python
1. 输入变量与输出变量
2. 数据类型
3. 连续变量与类别变量
```

###### 2 变量分析

```python
1. 单变量分析
	连续变量：
	  集中趋势：Mean、Median、Mode、Min、Max
	  分散度量：Range、Quartile、IQR、Variance、Standard Deviation、Skewness and Kurtosis
	  可视化：Histogram(直方图)、BoxPlot(箱型图)
	类别变量：
	  可视化：柱形图
2. 双变量分析
      连续型与连续型：绘制散点图和计算相关性（皮尔逊相关系数）
      类别型与类别型：双向表、卡方检验
      类别型与连续型：小提琴图（seaborn中violinplot()函数）
```



###### 3 缺失值处理

```python
1. 缺失值的产生和分类：
    完全随机丢失：抛硬币
    随机丢失：女性年龄缺失率
    不可预测因子导致的缺失：抛弃导致病人不适的诊断方法
    取决于自身的缺失：收入低不愿意透露
2. 处理方法
    删除：成列删除、成对删除
    填充：均值、众数、中位数
    预测模型填充：将数据集分为两部分，没有缺失的做训练集，缺失的做测试集
```

###### 4 异常值处理

```python
1. 产生的原因和影响
    数据输入误差
    测量误差
    实验误差
    人为故意误差
    数据处理误差
    采样误差
2. 异常值检测
    可视化：箱线图（IQR:[-1.5*IQR-1.5*IQR]）、直方图、散点图。
3. 异常值处理
    删除：直接drop
    转换：如对数转换会减轻由极值引起的变化
    填充：分自然形成或人为，预测、均值、。。。
    区别对待：大量缺失值，异常值为一组，非异常值为一组，分别建模。
```



###### 5 变量转换

```python
1. 目的
    非正态分布转换为正态分布，满足模型要求
2. 方法
    缩放比例、标准化
    非线性转换成线性：对数变换
    是倾斜分布对称：对数变换、取平方根或立方根
    变量分组：收入分类低中高 onehot编码
```

###### 6 新变量生成

```python
1. 目的
    产生的新变量与目标有更强的相关性
2. 方法
    创建派生变量
    创建哑变量
```

#### 2实际代码

###### 1 查看基本信息：

```python
1. data.info()
2. data.describe()
3. data.head()
```

###### 2 可视化 - 箱型图

```python
# 从图中发现数据存在较多偏离较大的异常值，可以考虑移除
    column = train_data.columns.tolist()[:39]  # 列表头
	fig = plt.figure(figsize=(80, 60), dpi=75)  # 指定绘图对象宽度和高度
	for i in range(38):
    	plt.subplot(7, 8, i + 1)  # 13行3列子图
    	sns.boxplot(train_data[column[i]], orient="v", width=0.5)  # 箱式图
    	plt.ylabel(column[i], fontsize=36)
	plt.show()
```

###### 3 异常值绘图

```python
# 异常值绘图
	通过岭回归找出异常值
```

###### 4 直方图与Q-Q图

```python
# 发现较多特征的分布不是正态分布，数据并不跟随对角线分布、后续可以使用数据变换对其进行处理。
    train_cols = 6
	train_rows = len(train_data.columns)
	plt.figure(figsize=(4*train_cols,4*train_rows))
	i=0
	for col in train_data.columns:
    	i+=1
    	ax=plt.subplot(train_rows,train_cols,i)
    	sns.distplot(train_data[col],fit=stats.norm)
    	i+=1
    	ax=plt.subplot(train_rows,train_cols,i)
    	res = stats.probplot(train_data[col], plot=plt)
	plt.tight_layout()
	plt.show()
```

###### 5 KDE

```python
# 对比训练集与测试集中同一特征的分布，，找出分布不一致的特征，可以剔除这些特征。
	dist_cols = 6
	dist_rows = len(test_data.columns)
	plt.figure(figsize=(4 * dist_cols, 4 * dist_rows))
	i = 1
	for col in test_data.columns:
    	ax = plt.subplot(dist_rows, dist_cols, i)
    	ax = sns.kdeplot(train_data[col], color="Red", shade=True)
    	ax = sns.kdeplot(test_data[col], color="Blue", shade=True)
    	ax.set_xlabel(col)
    	ax.set_ylabel("Frequency")
    	ax = ax.legend(["train", "test"])
    	i += 1
	plt.show()
```

###### 6 特征与Target回归关系图

```python
fcols = 6
frows = len(test_data.columns)
plt.figure(figsize=(5*fcols,4*frows))

i=0
for col in test_data.columns:
    i+=1
    ax=plt.subplot(frows,fcols,i)
    sns.regplot(x=col, y='target', data=train_data, ax=ax, 
                scatter_kws={'marker':'.','s':3,'alpha':0.3},
                line_kws={'color':'k'});
    plt.xlabel(col)
    plt.ylabel('target')
    
    i+=1
    ax=plt.subplot(frows,fcols,i)
    sns.distplot(train_data[col].dropna())
    plt.xlabel(col)
```

###### 7 特征与Target回归关系

```python

fcols = 6
frows = len(test_data.columns)
plt.figure(figsize=(5*fcols,4*frows))

i=0
for col in test_data.columns:
    i+=1
    ax=plt.subplot(frows,fcols,i)
    sns.regplot(x=col, y='target', data=train_data, ax=ax, 
                scatter_kws={'marker':'.','s':3,'alpha':0.3},
                line_kws={'color':'k'});
    plt.xlabel(col)
    plt.ylabel('target')
    
    i+=1
    ax=plt.subplot(frows,fcols,i)
    sns.distplot(train_data[col].dropna())
    plt.xlabel(col)
```

###### 8 特征与变量相关系数

```python
# 剔除分布不一致的相关变量后，计算剩余特征与target之间的相关系数
# 相关系数主要用于判断线性相关，对于目标target如果存在更复杂的函数形式的影响，可以用树模型的特征重要性选择
pd.set_option('display.max_columns', 10)
pd.set_option('display.max_rows', 10)
data_train1 = train_data.drop(['V5', 'V9', 'V11', 'V17', 'V22', 'V28'], 
                              axis=1)
train_corr = data_train1.corr()
train_corr

# 相关系数热力图
ax = plt.subplots(figsize=(20, 16))#调整画布大小
ax = sns.heatmap(train_corr, vmax=.8, square=True, annot=True)#画热力图   annot=True 显示系数

# K个最相关特征
k = 10  # number of variables for heatmap
cols = train_corr.nlargest(k, 'target')['target'].index

cm = np.corrcoef(train_data[cols].values.T)
hm = plt.subplots(figsize=(10, 10))  #调整画布大小
hm = sns.heatmap(train_data[cols].corr(), annot=True, square=True)
plt.show()

# 相关系数大于0.5的特征变量
threshold = 0.5

corrmat = train_data.corr()
top_corr_features = corrmat.index[abs(corrmat["target"]) > threshold]
plt.figure(figsize=(10, 10))
g = sns.heatmap(train_data[top_corr_features].corr(),
                annot=True,
                cmap="RdYlGn")

# 移除部分特征
threshold = 0.5

# 相关系矩阵
corr_matrix = data_train1.corr().abs()
drop_col=corr_matrix[corr_matrix["target"]<threshold].index
#data_all.drop(drop_col, axis=1, inplace=True)
```

###### 9 Box-Cox变换

```python
# 由于线性回归是基于正态分布的，在进行统计分析时，需要将数据转换成使其符合正态分布。
# Box-Cox变换能使得线性回归满足线性、正态性、独立性、以及方差齐性同时又不丢失信息。
# Box-Cox前需要做归一化
# 合并训练集与测试集
drop_columns = ['V5','V9','V11','V17','V22','V28']

# 合并训练和测试数据集
train_x =  train_data.drop(['target'], axis=1)
#data_all=pd.concat([train_data,test_data],axis=0,ignore_index=True)
data_all = pd.concat([train_x,test_data]) 
data_all.drop(drop_columns,axis=1,inplace=True)

# 对合并后的每列进行归一化
cols_numeric=list(data_all.columns)
def scale_minmax(col):
    return (col-col.min())/(col.max()-col.min())
data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax,axis=0)
data_all[cols_numeric].describe()

# 分别归一化，建立在测试集与训练集分布一致的情况下，能加快速度
train_data_process = train_data[cols_numeric]
train_data_process = train_data_process[cols_numeric].apply(scale_minmax,axis=0)
test_data_process = test_data[cols_numeric]
test_data_process = test_data_process[cols_numeric].apply(scale_minmax,axis=0)

# Box-Cox变换分析
cols_numeric_left = cols_numeric[0:13]
cols_numeric_right = cols_numeric[13:]
train_data_process = pd.concat([train_data_process, train_data['target']],
                               axis=1)

fcols = 6
frows = len(cols_numeric_left)
plt.figure(figsize=(4 * fcols, 4 * frows))
i = 0

for var in cols_numeric_left:
    dat = train_data_process[[var, 'target']].dropna()

    i += 1
    plt.subplot(frows, fcols, i)
    sns.distplot(dat[var], fit=stats.norm)
    plt.title(var + ' Original')
    plt.xlabel('')

    i += 1
    plt.subplot(frows, fcols, i)
    _ = stats.probplot(dat[var], plot=plt)
    plt.title('skew=' + '{:.4f}'.format(stats.skew(dat[var])))
    plt.xlabel('')
    plt.ylabel('')

    i += 1
    plt.subplot(frows, fcols, i)
    plt.plot(dat[var], dat['target'], '.', alpha=0.5)
    plt.title('corr=' +
              '{:.2f}'.format(np.corrcoef(dat[var], dat['target'])[0][1]))

    i += 1
    plt.subplot(frows, fcols, i)
    trans_var, lambda_var = stats.boxcox(dat[var].dropna() + 1)
    trans_var = scale_minmax(trans_var)
    sns.distplot(trans_var, fit=stats.norm)
    plt.title(var + ' Tramsformed')
    plt.xlabel('')

    i += 1
    plt.subplot(frows, fcols, i)
    _ = stats.probplot(trans_var, plot=plt)
    plt.title('skew=' + '{:.4f}'.format(stats.skew(trans_var)))
    plt.xlabel('')
    plt.ylabel('')

    i += 1
    plt.subplot(frows, fcols, i)
    plt.plot(trans_var, dat['target'], '.', alpha=0.5)
    plt.title('corr=' +
              '{:.2f}'.format(np.corrcoef(trans_var, dat['target'])[0][1]))
```





