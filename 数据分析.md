# 《 数据挖掘的一般流程 》



## 一、 业务理解

###### 1 业务背景

- ```python
  1. 数据说明
  2. 核心特征
  3. 可调参数
  4. 采集时间
  ```


###### 2 业务目标

- ```python
  1. 目标结果
  ```


###### 3 数据概览

- ```python
  1. 数据量纲
  2. 参数类别
  3. 数据属性
  4. 数据描述
  ```


###### 4 评估指标

- ```python
  1. 回归：mse
  2. 分类：F1 score
  ```


###### 5 业务模型

- ```python
  1. 分类模型：决策树、随机森林
  2. 回归模型：线性回归
  ```


## 二、数据探索

#### 1 理论知识

###### 1 变量识别

- ```python
  1. 输入变量、输出变量
  2. 数据类型
  3. 连续变量、类别变量、离散变量
  ```


###### 2 变量分析

- ```python
  1. 单变量分析
  	连续变量：
  	  集中趋势：Mean、Median、Mode、Min、Max
  	  分散度量：Range、Quartile、IQR、Variance、Standard Deviation、Skewness and Kurtosis
  	  可视化：Histogram(直方图)、BoxPlot(箱型图)
  	类别变量：
  	  可视化：柱形图
  2. 双变量分析
        连续型与连续型：绘制散点图、计算相关性（皮尔逊相关系数）
        类别型与类别型：双向表、卡方检验
        类别型与连续型：小提琴图（seaborn中violinplot()函数）
  
  # 相关系数
  import numpy as np
  X = np.array([65, 72, 78, 65, 72, 70, 65, 68])
  Y = np.array([72, 69, 79, 69, 84, 75, 60, 73])
  np.corrcoef(X, Y)
  
  # 卡方检验
  from sklearn.datasets import load_iris
  from sklearn.feature_selection import SelectKBest
  from sklearn.feature_selection import chi2
  iris = load_iris()
  X, y = iris.data, iris.target
  chiValues = chi2(X, y)
  X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
  ```




###### 3 缺失值处理

- ```python
  1. 缺失值的产生和分类
      完全随机丢失：抛硬币
      随机丢失：女性年龄缺失率
      不可预测因子导致的缺失：抛弃导致病人不适的诊断方法
      取决于自身的缺失：收入低不愿意透露
  2. 处理方法
      删除：成列删除、成对删除
      填充：均值、众数、中位数
      模型预测填充：将数据集分为两部分，没有缺失的做训练集，缺失的做测试集
  ```


###### 4 异常值处理

- ```python
  1. 产生的原因和影响
      数据输入误差
      测量误差
      实验误差
      人为故意误差
      数据处理误差
      采样误差
  2. 异常值检测
      可视化：箱线图（IQR:[-1.5*IQR-1.5*IQR]）、直方图、散点图。
  3. 异常值处理
      删除：直接drop
      转换：如对数转换会减轻由极值引起的变化
      填充：分自然形成或人为，预测、均值、。。。
      区别对待：大量缺失值，异常值为一组，非异常值为一组，分别建模。
  ```


###### 5 变量转换

- ```python
  1. 目的
      非正态分布转换为正态分布，满足模型要求
  2. 方法
      缩放比例、标准化
      非线性转换成线性：对数变换
      是倾斜分布对称：对数变换、取平方根或立方根
      变量分组：收入分类低中高 onehot编码
  ```


###### 6 新变量生成

- ```python
  1. 目的
      产生的新变量与目标有更强的相关性
  2. 方法
      创建派生变量
      创建哑变量
  ```


#### 2 实际代码

###### 1 查看基本信息：

- ```python
  # 查看基本信息
  data.info() # 样本数、变量数、变量类型、缺失值数、内存占用
  data.describe() # 行计数、均值、方差、最小值、最大值、百分位数
  data.head() # 数据形式
  ```


###### 2 箱型图

- ```python
  # 箱型图，检测异常值
  colunms = train_data.columns.tolist()[:39] # 获取所有列名
  fig=plt.figure(figsize=(80,60),dpi=75) # 设置画布大小
  for i in range(38):
      plt.subplot(7,8,i+1) # 7行8列的第i+1个图
      bp=sns.boxplot(train_data[colunms[i]],orient="v",width=0.5) # 箱形图
      plt.ylabel(colunms[i],fontsize=16) # y轴标签
  plt.show()
  
  # 存在许多偏离较大的异常值，可以考虑移除
  ```


###### 3 异常值绘图

- ```python
  # 异常值绘图
  	通过岭回归找出异常值
  ```


###### 4 直方图与Q-Q图

- ```python
  # 直方图和Q-Q图，检测非偏正态分布
  trainc_cols=6
  train_rows=len(train_data.columns)
  plt.figure(figsize=(4*trainc_cols,4*train_rows))
  
  i=0
  for col in train_data.columns:
      i+=1
      ax=plt.subplot(train_rows,trainc_cols,i)
      sns.distplot(train_data[col],fit=stats.norm)
  
      i+=1
      ax=plt.subplot(train_rows,trainc_cols,i+trainc_cols)
      res=stats.probplot(train_data[col],plot=plt)
  plt.tight_layout()
  plt.show()
  
  # 很多特征变量分布不是正态分布，需要进行转换
  ```


###### 5 KDE核密度估计图

- ```python
  # KDE分布图，检测偏度和峰度
  dist_cols=6
  dist_rows=len(test_data.columns) # 训练集与测试集对比，采用测试集的列数
  plt.figure(figsize=(4*dist_cols,4*dist_rows))
  i=1
  for col in test_data.columns:
      ax=plt.subplot(dist_rows,dist_cols,i)
      ax=sns.kdeplot(train_data[col],color='Red',shade=True)
      ax=sns.kdeplot(test_data[col],color='Blue',shade=True)
      ax.set_xlabel(col)
      ax.set_ylabel('Frequency')
      ax=ax.legend(['train','test'])
      i+=1
  plt.show()
  
  # V5、V9、V11、V17、V22、V28在训练集与测试集当中分布不一致，这会导致模型的泛化性能下降，需要删除此类特征
  ```


###### 6 特征与Target回归关系

- ```python
  # 线性回归关系图，检测特征与标签的线性关系
  fcols = 6
  frows = len(test_data.columns)
  plt.figure(figsize=(4*fcols,4*frows))
  i=0
  for col in test_data.columns:
      i+=1
      ax=plt.subplot(frows,fcols,i)
      sns.regplot(x=col,y='target',data=train_data,ax=ax,scatter_kws={'marker':'.','s':3,'alpha':0.3},line_kws={'color':'k'})
      plt.xlabel(col)
      plt.ylabel("target")
      i+=1
  
      ax=plt.subplot(frows,fcols,i)
      sns.distplot(train_data['target'].dropna())
      plt.xlabel('col')
  
  """
  这行代码使用seaborn模块的regplot函数绘制了一个以x=col和y='target'为自变量和因变量的散点图，并在散点图上拟合了一条线性回归线。具体解释如下：
  
  x=col表示将该行代码所在的列col作为自变量。
  y='target'表示将'target'作为因变量。
  data=train_data表示使用train_data中的数据进行绘图。
  ax=ax表示将该子图添加到ax对象中，以便在多个子图中进行排列。
  scatter_kws={'marker':'.', 's':3, 'alpha':0.3}表示设置散点图的样式参数。其中，marker表示散点图的标记，这里使用小圆点'.'；s表示标记的大小，这里设置为3；alpha表示标记的透明度，这里设置为0.3。
  line_kws={'color':'k'}表示设置拟合直线的样式参数。其中，color表示直线的颜色，这里设置为黑色'k'。
  综上所述，该行代码绘制了一个以col为自变量，'target'为因变量的散点图，并在图像上拟合了一条线性回归线，同时设置了散点图和拟合直线的样式参数。
  """
  ```


###### 7 特征与变量相关系数

- ```python
  # 剔除分布不一致的相关变量后，计算剩余特征与target之间的相关系数
  # 相关系数主要用于判断线性相关，对于目标target如果存在更复杂的函数形式的影响，可以用树模型的特征重要性选择
  pd.set_option('display.max_columns', 10)
  pd.set_option('display.max_rows', 10)
  data_train1 = train_data.drop(['V5', 'V9', 'V11', 'V17', 'V22', 'V28'], 
                                axis=1)
  train_corr = data_train1.corr()
  train_corr
  
  # 相关系数热力图
  ax = plt.subplots(figsize=(20, 16))#调整画布大小
  ax = sns.heatmap(train_corr, vmax=.8, square=True, annot=True)#画热力图   annot=True 显示系数
  
  # K个最相关特征
  k = 10
  cols = train_corr.nlargest(k, 'target')['target'].index # nlargest()表示取出最大的k个数值，index表示取出对应的索引
  cm = np.corrcoef(train_data[cols].values.T) # 计算这k个特征变量之间的相关系数
  hm = plt.subplots(figsize=(10, 10)) # 调整画布大小
  hm = sns.heatmap(train_data[cols].corr(),annot=True,square=True) # 画热力图
  plt.show()
  
  ## 寻找与target相关系数大于0.5的特征变量
  threshold = 0.5
  corrmat = train_data.corr() # 重新计算全部特征变量之间的相关系数
  top_corr_features=corrmat.index[abs(corrmat["target"])>threshold]   # abs()表示取绝对值
  plt.figure(figsize=(10,10))
  g=sns.heatmap(train_data[top_corr_features].corr(),annot=True,cmap="RdYlGn")    # 画出相关系数大于0.5的特征变量之间的热力图
  
  
  #用相关系数阈值移除相关特征
  threshold = 0.5
  corr_matrix = data_train.corr().abs()  # 计算相关系数的绝对值
  drop_col = corr_matrix[corr_matrix["target"]<threshold].index  # 找出相关系数小于阈值的特征变量
  data_train.drop(drop_col, axis=1, inplace=True)  # 删除相关系数小于阈值的特征变量
  ```


###### 8 Box-Cox变换

- ```python
  # 由于线性回归是基于正态分布的，在进行统计分析时，需要将数据转换成使其符合正态分布。
  # Box-Cox变换能使得线性回归满足线性、正态性、独立性、以及方差齐性同时又不丢失信息。
  # Box-Cox前需要做归一化
  # 合并训练集与测试集
  drop_columns = ['V5','V9','V11','V17','V22','V28']
  
  # 合并训练和测试数据集
  train_x =  train_data.drop(['target'], axis=1)
  #data_all=pd.concat([train_data,test_data],axis=0,ignore_index=True)
  data_all = pd.concat([train_x,test_data]) 
  data_all.drop(drop_columns,axis=1,inplace=True)
  
  # 对合并后的每列进行归一化
  cols_numeric=list(data_all.columns)
  def scale_minmax(col):
      return (col-col.min())/(col.max()-col.min())
  data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax,axis=0)
  data_all[cols_numeric].describe()
  
  # 分别归一化，建立在测试集与训练集分布一致的情况下，能加快速度
  train_data_process = train_data[cols_numeric]
  train_data_process = train_data_process[cols_numeric].apply(scale_minmax,axis=0)
  test_data_process = test_data[cols_numeric]
  test_data_process = test_data_process[cols_numeric].apply(scale_minmax,axis=0)
  
  # Box-Cox变换分析
  cols_numeric_left = cols_numeric[0:13]
  cols_numeric_right = cols_numeric[13:]
  train_data_process = pd.concat([train_data_process, train_data['target']], axis=1)# 将target加入训练集（已经归一化的训练集）
  
  fcols = 6
  frows = len(cols_numeric_left)
  plt.figure(figsize=(4 * fcols, 4 * frows))
  i = 0
  
  for var in cols_numeric_left:
      dat = train_data_process[[var, 'target']].dropna()
  
      i += 1
      plt.subplot(frows, fcols, i)
      sns.distplot(dat[var], fit=stats.norm)
      plt.title(var + ' Original')
      plt.xlabel('')
  
      i += 1
      plt.subplot(frows, fcols, i)
      _ = stats.probplot(dat[var], plot=plt)
      plt.title('skew=' + '{:.4f}'.format(stats.skew(dat[var])))
      plt.xlabel('')
      plt.ylabel('')
  
      i += 1
      plt.subplot(frows, fcols, i)
      plt.plot(dat[var], dat['target'], '.', alpha=0.5)
      plt.title('corr=' + '{:.2f}'.format(np.corrcoef(dat[var], dat['target'])[0][1]))
  
      i += 1
      plt.subplot(frows, fcols, i)
      trans_var, lambda_var = stats.boxcox(dat[var].dropna() + 1)
      trans_var = scale_minmax(trans_var)  # 应用boxcox变换后继续归一化，方便对比
      sns.distplot(trans_var, fit=stats.norm)
      plt.title(var + ' Tramsformed')
      plt.xlabel('')
  
      i += 1
      plt.subplot(frows, fcols, i)
      _ = stats.probplot(trans_var, plot=plt)
      plt.title('skew=' + '{:.4f}'.format(stats.skew(trans_var)))
      plt.xlabel('')
      plt.ylabel('')
  
      i += 1
      plt.subplot(frows, fcols, i)
      plt.plot(trans_var, dat['target'], '.', alpha=0.5)
      plt.title('corr=' + '{:.2f}'.format(np.corrcoef(trans_var, dat['target'])[0][1]))
  ```




## 三、特征工程

## 四、模型训练

## 五、模型评估

## 六、特征优化

## 七、模型融合





